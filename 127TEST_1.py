import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss
from itertools import product
import os
import warnings

# å¿½ç•¥ä¸€äº›é™¤é›¶è­¦å‘Š
warnings.filterwarnings('ignore')

# ==========================================
#               1. å®éªŒé…ç½®åŒº (User Config)
# ==========================================

# 1. åœ¨è¿™é‡Œé…ç½®æ‰€æœ‰ä½ æƒ³æµ‹è¯•çš„ Q çŸ©é˜µæ–‡ä»¶è·¯å¾„
# æ ¼å¼ï¼š{"æ˜¾ç¤ºåç§°": "æ–‡ä»¶è·¯å¾„"}
Q_MATRIX_LIST = {
    "1. LLMç­–ç•¥2": r"LLM_Q_Generate\outputs\4+10_1_æ”¹_results\DINA_Q_Matrix_4+10_æ”¹.xlsx",
    "2. LLMç­–ç•¥3": r"LLM_Q_Generate\outputs\4+10_2_8_results\DINA_Q_Matrix_4+10_2_8.xlsx",
    "3. LLMç­–ç•¥4": r"LLM_Q_Generate\outputs\4+10_3_results\DINA_Q_Matrix_3.xlsx",
    "4. ä¸“å®¶æ‰‹å·¥é™ç»´ (14ç»´)": r"Artificial_Q_process\QçŸ©é˜µ_æ‰‹å·¥æ ‡æ³¨åˆå¹¶14ä¸ª.xlsx",
    "5. è€ç‰ˆæœ¬ V1 (10ç»´-è¯é¢‘)": r"516matrix.xlsx" 
}

# 2. æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_PATH = "cleaned_data_20250326_0931.csv"
GROUP_PATH = "optimal_student_groups_leiden.csv"

# 3. å®éªŒè¶…å‚æ•°
TRAIN_RATIO = 0.5       # 80% è®­ç»ƒï¼Œ20% æµ‹è¯•
RANDOM_SEED = 42        # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
MAX_KNOWLEDGE = 15      # ã€å…³é”®ã€‘å¦‚æœçŸ¥è¯†ç‚¹è¶…è¿‡è¿™ä¸ªæ•°ï¼Œè‡ªåŠ¨æˆªæ–­ï¼ˆé˜²æ­¢æ­»æœºï¼‰

# ==========================================
#           2. DINA æ ¸å¿ƒç®—å­ (Core)
# ==========================================

def compute_eta(Q, A):
    kowns = np.sum(Q * Q, axis=0)
    cross = np.dot(A, Q)
    eta = np.ones(shape=(A.shape[0], Q.shape[1]))
    eta[cross < kowns] = 0
    return eta

def compute_propa(eta, s, g):
    propa = (g ** (1 - eta)) * ((1 - s) ** eta)
    propa = np.clip(propa, 1e-10, 1 - 1e-10) # æ•°å€¼ç¨³å®š
    return propa

def compute_gamma(X, pi, propa):
    log_pj = np.log(propa)
    log_qj = np.log(1 - propa)
    log_pi = np.log(pi)
    # å…³é”®ï¼šåˆ©ç”¨çŸ©é˜µä¹˜æ³•åŠ é€Ÿè®¡ç®—åéªŒ
    gamma = np.exp(np.dot(X, log_pj.T) + np.dot((1 - X), log_qj.T) + log_pi)
    gamma_sum = np.sum(gamma, axis=1, keepdims=True)
    gamma_sum[gamma_sum == 0] = 1e-15
    gamma = gamma / gamma_sum
    return gamma

def compute_theta(X, gamma, eta):
    I0 = np.dot(gamma, 1 - eta)
    I1 = np.dot(gamma, eta)
    R0 = I0 * X
    R1 = I1 * X
    
    I0_sum = np.sum(I0, axis=0)
    I1_sum = np.sum(I1, axis=0)
    R0_sum = np.sum(R0, axis=0)
    R1_sum = np.sum(R1, axis=0)
    
    # é˜²æ­¢åˆ†æ¯ä¸º0
    I0_sum[I0_sum <= 0] = 1e-15
    I1_sum[I1_sum <= 0] = 1e-15
    
    g = R0_sum / I0_sum
    s = (I1_sum - R1_sum) / I1_sum
    pi = np.sum(gamma, axis=0) / gamma.shape[0]
    
    return np.clip(pi, 1e-15, 1-1e-15), np.clip(s, 0.001, 0.999), np.clip(g, 0.001, 0.999)

# --- è®­ç»ƒå‡½æ•° (Full EM) ---
def train_dina(X, Q, max_iter=50, tol=1e-3):
    n_stu, n_items = X.shape
    n_kno = Q.shape[0]
    
    # åˆå§‹åŒ–
    s = np.random.uniform(0.1, 0.3, n_items)
    g = np.random.uniform(0.1, 0.3, n_items)
    
    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æŒæ¡æ¨¡å¼ (2^K)
    # æ³¨æ„ï¼šå¦‚æœ K > 20 è¿™é‡Œä¼šçˆ†å†…å­˜ï¼Œä½†æˆ‘ä»¬å‰é¢çš„ MAX_KNOWLEDGE ä¼šé˜²ä½å®ƒ
    A_all = np.array(list(product([0, 1], repeat=n_kno)))
    pi = np.ones(A_all.shape[0]) / A_all.shape[0] # å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
    
    for t in range(max_iter):
        eta = compute_eta(Q, A_all)
        propa = compute_propa(eta, s, g)
        gamma = compute_gamma(X, pi, propa)
        pi_new, s_new, g_new = compute_theta(X, gamma, eta)
        
        # æ£€æŸ¥æ”¶æ•›
        diff = max(np.max(np.abs(pi_new - pi)), np.max(np.abs(s_new - s)), np.max(np.abs(g_new - g)))
        pi, s, g = pi_new, s_new, g_new
        if diff < tol:
            break
            
    return {"s": s, "g": g, "pi": pi, "A_all": A_all}

# --- é¢„æµ‹å‡½æ•° (Inference Only) ---
def predict_dina(X_test, Q, model_params):
    s, g, pi, A_all = model_params["s"], model_params["g"], model_params["pi"], model_params["A_all"]
    
    # 1. è®¡ç®—ç†è®ºç­”é¢˜æ¦‚ç‡
    eta = compute_eta(Q, A_all)
    propa = compute_propa(eta, s, g)
    
    # 2. E-Step: æ¨æ–­æµ‹è¯•é›†å­¦ç”Ÿçš„èƒ½åŠ›åˆ†å¸ƒ (Gamma)
    gamma_test = compute_gamma(X_test, pi, propa)
    
    # 3. é¢„æµ‹ç­”é¢˜è¡Œä¸º (æ¦‚ç‡çŸ©é˜µ)
    # å­¦ç”Ÿçš„é¢„æµ‹ç­”é¢˜æ¦‚ç‡ = sum(è¯¥å­¦ç”Ÿå±äºæ¨¡å¼kçš„æ¦‚ç‡ * æ¨¡å¼kç­”å¯¹è¯¥é¢˜çš„æ¦‚ç‡)
    X_pred_prob = np.dot(gamma_test, propa)
    
    return X_pred_prob

# ==========================================
#      3. æ™ºèƒ½ Q çŸ©é˜µæ„å»º (Smart Builder)
# ==========================================

def build_smart_q_matrix(file_path, group_qs_ids, max_k=15):
    """
    æ™ºèƒ½åŠ è½½å‡½æ•°ï¼š
    1. è¯»å– Q çŸ©é˜µæ–‡ä»¶
    2. è‡ªåŠ¨åŒ¹é…æœ¬ç»„é¢˜ç›®
    3. ã€å…³é”®ã€‘å¦‚æœçŸ¥è¯†ç‚¹è¿‡å¤šï¼Œè‡ªåŠ¨ç­›é€‰ Top-K é«˜é¢‘çŸ¥è¯†ç‚¹ï¼Œé˜²æ­¢çˆ†ç‚¸
    """
    try:
        # 1. è¯»å–æ–‡ä»¶
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_excel(file_path)
            
        # å°è¯•å°†ç¬¬ä¸€åˆ—æˆ–åä¸º id/qs_id çš„åˆ—è®¾ä¸ºç´¢å¼•
        cols_lower = [c.lower() for c in df.columns]
        if 'id' in cols_lower:
            df = df.set_index(df.columns[cols_lower.index('id')])
        elif 'qs_id' in cols_lower:
            df = df.set_index(df.columns[cols_lower.index('qs_id')])
        elif 'é¢˜ç›®id' in cols_lower:
             df = df.set_index(df.columns[cols_lower.index('é¢˜ç›®id')])
        else:
            # é»˜è®¤ç¬¬ä¸€åˆ—æ˜¯ ID
            df = df.set_index(df.columns[0])
            
        # ç»Ÿä¸€ç´¢å¼•ä¸ºå­—ç¬¦ä¸²
        df.index = df.index.astype(str)
        
        # 2. è¯†åˆ«çŸ¥è¯†ç‚¹åˆ— (æ•°å­—åˆ—ï¼Œä¸”ä¸æ˜¯å…¨æ˜¯0æˆ–å…¨æ˜¯1ä¹‹å¤–çš„ä¹±ä¸ƒå…«ç³Ÿçš„æ•°)
        # ç®€å•é€»è¾‘ï¼šé€‰å–æ‰€æœ‰æ•°å€¼ç±»å‹çš„åˆ—ä½œä¸ºå€™é€‰
        numeric_df = df.select_dtypes(include=[np.number]).fillna(0)
        
        # 3. ç­›é€‰å‡ºæœ¬ç»„æ¶‰åŠçš„é¢˜ç›®
        valid_qs = [q for q in group_qs_ids if q in df.index]
        if not valid_qs:
            print(f"   [Error] è¯¥ Q çŸ©é˜µæœªåŒ…å«æœ¬ç»„ä»»ä½•é¢˜ç›®ï¼")
            return None, None
            
        subset_df = numeric_df.loc[valid_qs]
        
        # 4. ã€æ ¸å¿ƒä¼˜åŒ–ã€‘çŸ¥è¯†ç‚¹é™ç»´é€»è¾‘
        current_k = subset_df.shape[1]
        
        if current_k > max_k:
            print(f"   [ä¼˜åŒ–] æ£€æµ‹åˆ°çŸ¥è¯†ç‚¹ç»´åº¦ K={current_k} > {max_k}ï¼Œæ­£åœ¨æ‰§è¡Œæ™ºèƒ½é™ç»´...")
            # è®¡ç®—æ¯ä¸ªçŸ¥è¯†ç‚¹çš„è¦†ç›–ç‡ï¼ˆåœ¨æœ¬ç»„é¢˜ç›®ä¸­ï¼‰
            coverage = subset_df.sum(axis=0)
            # é€‰å‡º Top-K
            top_cols = coverage.nlargest(max_k).index
            final_df = subset_df[top_cols]
            # å†æ¬¡è½¬ä¸º 0/1 (é˜²æ­¢ excel é‡Œå†™äº† 2, 3 è¿™ç§æƒé‡)
            Q_matrix = (final_df.values > 0).astype(int).T # è½¬ç½®ä¸º (K, Items)
            print(f"   [æˆåŠŸ] å·²é™ç»´è‡³ Top {max_k} çŸ¥è¯†ç‚¹")
        else:
            Q_matrix = (subset_df.values > 0).astype(int).T
            
        # ç§»é™¤å…¨é›¶è¡Œï¼ˆæœ‰äº›çŸ¥è¯†ç‚¹å¯èƒ½åœ¨æœ¬ç»„é¢˜ç›®é‡Œæ ¹æœ¬æ²¡è€ƒï¼‰
        valid_k_idx = np.where(Q_matrix.sum(axis=1) > 0)[0]
        Q_matrix = Q_matrix[valid_k_idx, :]
        
        return Q_matrix, valid_qs
        
    except Exception as e:
        print(f"   [åŠ è½½å¤±è´¥] {file_path}: {e}")
        return None, None

# ==========================================
#               4. ä¸»å®éªŒæµç¨‹
# ==========================================

def main():
    print(f"ğŸš€ å¼€å§‹å¤šè½®å®éªŒ | è®­ç»ƒé›†: {TRAIN_RATIO*100}% | MAX_K: {MAX_KNOWLEDGE}")
    print("=" * 65)
    
    # 1. åŠ è½½åŸºç¡€æ•°æ®
    try:
        data_df = pd.read_csv(DATA_PATH)
        data_df['qs_id'] = data_df['qs_id'].astype(str)
        group_df = pd.read_csv(GROUP_PATH)
        print("âœ… åŸºç¡€æ•°æ®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶ç¼ºå¤±: {e}")
        return

    results = []

    # 2. éå† Q çŸ©é˜µåˆ—è¡¨
    for q_name, q_path in Q_MATRIX_LIST.items():
        if not os.path.exists(q_path):
            print(f"\nâš ï¸ è·³è¿‡ {q_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            continue
            
        print(f"\nğŸ“‚ æ­£åœ¨è¯„æµ‹: [{q_name}]")
        print("-" * 40)
        
        # 3. éå†ç»„åˆ« (Group 0, 1, 2)
        target_groups = [0, 1, 2]
        
        for grp_id in target_groups:
            # å‡†å¤‡è¯¥ç»„çš„å­¦ç”Ÿç­”é¢˜æ•°æ® X
            stu_ids = group_df[group_df['group'] == grp_id]['student_id'].values
            if len(stu_ids) < 10: continue # å¿½ç•¥å°æ ·æœ¬
            
            grp_records = data_df[data_df['student_id'].isin(stu_ids)]
            # è½¬ä¸ºçŸ©é˜µå½¢å¼: è¡Œ=å­¦ç”Ÿ, åˆ—=é¢˜ç›®
            X_df = grp_records.pivot_table(index='student_id', columns='qs_id', values='qs_validity', fill_value=0)
            group_qs_ids = X_df.columns.astype(str).tolist()
            
            # --- æ™ºèƒ½æ„å»º Q çŸ©é˜µ ---
            Q, valid_qs = build_smart_q_matrix(q_path, group_qs_ids, max_k=MAX_KNOWLEDGE)
            
            if Q is None or Q.shape[0] == 0:
                print(f"   Group {grp_id}: Q çŸ©é˜µæ„å»ºå¤±è´¥æˆ–æ— åŒ¹é…é¢˜ç›®")
                continue
                
            # å¯¹é½æ•°æ®ï¼šåªå– Q çŸ©é˜µä¸­å­˜åœ¨çš„é¢˜ç›®
            X_aligned = X_df[valid_qs].values
            
            # --- åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›† ---
            try:
                X_train, X_test = train_test_split(X_aligned, train_size=TRAIN_RATIO, random_state=RANDOM_SEED)
            except ValueError:
                print(f"   Group {grp_id}: æ ·æœ¬ä¸è¶³æ— æ³•åˆ’åˆ†ï¼Œè·³è¿‡")
                continue
            
            # --- é˜¶æ®µ 1: è®­ç»ƒ (Learning) ---
            # ä½¿ç”¨è®­ç»ƒé›†å­¦ä¹  s, g, pi
            model = train_dina(X_train, Q, max_iter=30)
            
            # --- é˜¶æ®µ 2: æµ‹è¯• (Inference) ---
            # ä½¿ç”¨å­¦ä¹ åˆ°çš„å‚æ•°é¢„æµ‹æµ‹è¯•é›†
            X_test_pred_prob = predict_dina(X_test, Q, model)
            
            # --- è¯„ä¼° ---
            X_test_pred_bin = (X_test_pred_prob >= 0.5).astype(int)
            acc = accuracy_score(X_test.flatten(), X_test_pred_bin.flatten())
            loss = log_loss(X_test.flatten(), X_test_pred_prob.flatten(), labels=[0,1])
            
            print(f"   Group {grp_id} (K={Q.shape[0]}): Test Acc = {acc:.4f} | LogLoss = {loss:.4f}")
            
            results.append({
                "Matrix": q_name,
                "Group": grp_id,
                "Knowledge_Dim": Q.shape[0],
                "Test_Accuracy": acc,
                "Test_LogLoss": loss
            })

    # 4. æœ€ç»ˆæ±‡æ€»è¾“å‡º
    print("\n" + "="*65)
    print("ğŸ† æœ€ç»ˆå®éªŒæŠ¥å‘Š (æŒ‰æµ‹è¯•é›†å‡†ç¡®ç‡æ’åº)")
    print("="*65)
    if results:
        res_df = pd.DataFrame(results)
        # è°ƒæ•´åˆ—é¡ºåº
        res_df = res_df[["Matrix", "Group", "Knowledge_Dim", "Test_Accuracy", "Test_LogLoss"]]
        print(res_df.sort_values(by="Test_Accuracy", ascending=False).to_string(index=False))
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        res_df.to_csv("final_experiment_results.csv", index=False)
        print("\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ final_experiment_results.csv")
    else:
        print("æœªäº§ç”Ÿæœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„é…ç½®ã€‚")

if __name__ == "__main__":
    main()