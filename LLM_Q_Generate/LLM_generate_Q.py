import pandas as pd
import json
import time
import requests
import os

# ğŸŒŸ ä»åŒçº§ç›®å½•å¯¼å…¥æç¤ºè¯
from prompt2_5 import prompt

# ================= é…ç½®åŒº =================
# DeepSeek API é…ç½®
API_KEY = "sk-2636e69fcc0744fa8b975e2b82eaa345"
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"

# æ–‡ä»¶è·¯å¾„é…ç½®
INPUT_FILE = "æ‰€æœ‰é¢˜ç›®.xlsx"
NOTE = "4+10_2_7"
OUTPUT_DIR = f"./outputs/{NOTE}_results"  # ğŸ“‚ æŒ‡å®šè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
BATCH_SIZE = 20

# ğŸ§ª æµ‹è¯•æ¨¡å¼é…ç½®
TEST_MODE = False
TEST_K = 40  
RANDOM_SEED = 42  

# ğŸ’¾ ä¸´æ—¶å­˜æ¡£ (è‡ªåŠ¨ä¿å­˜åœ¨è¾“å‡ºç›®å½•ä¸‹)
TEMP_FILENAME = f"temp_saved_tags_{NOTE}.csv"

# ğŸ“š çŸ¥è¯†ç‚¹å®šä¹‰ (ç”¨äºç”Ÿæˆåˆ—åå’Œäººå·¥å¯¹ç…§æ–‡æœ¬)
KNOWLEDGE_MAP = {
    1: "K1_ä»ªå™¨æ“ä½œ", 2: "K2_ç”µè·¯æ„å»º", 3: "K3_æ•…éšœæ’æŸ¥", 4: "K4_æ•°æ®å¤„ç†",
    5: "K5_ç›´æµåˆ†æ", 6: "K6_æš‚æ€è¿‡ç¨‹", 7: "K7_äº¤æµç¨³æ€", 8: "K8_é¢‘ç‡å“åº”",
    9: "K9_è°æŒ¯ç†è®º", 10: "K10_åŠå¯¼ä½“", 11: "K11_æ”¾å¤§ç”µè·¯", 12: "K12_è¿æ”¾åº”ç”¨",
    13: "K13_æŒ¯è¡åé¦ˆ", 14: "K14_å˜å‹å™¨ä¸‰ç›¸"
}
# ========================================

def call_deepseek_api(batch_df):
    """
    è°ƒç”¨ DeepSeek API è¿›è¡Œæ‰“æ ‡
    """
    questions_text = ""
    for _, row in batch_df.iterrows():
        questions_text += f"ID: {row['id']}\né¢˜ç›®: {row['qs_title']}\nç« èŠ‚: {row['section_name']}\n---\n"

    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": f"è¯·å¯¹ä»¥ä¸‹é¢˜ç›®è¿›è¡Œæ‰“æ ‡ï¼Œä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›:\n\n{questions_text}"}
    ]

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
        "temperature": 0.1,
        "max_tokens": 4096,
        "response_format": {"type": "json_object"}
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    retries = 3
    for attempt in range(retries):
        try:
            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result_json = response.json()
            content = result_json['choices'][0]['message']['content']
            content = content.replace("```json", "").replace("```", "").strip()
            
            data = json.loads(content)
            
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, list):
                        return value
                print(f"è­¦å‘Š: è¿”å›çš„ JSON ç»“æ„ä¸ç¬¦åˆé¢„æœŸ: {data.keys()}")
                return []
            elif isinstance(data, list):
                return data
                
        except Exception as e:
            print(f"âš ï¸ API è¯·æ±‚å‡ºé”™ (å°è¯• {attempt+1}/{retries}): {e}")
            time.sleep(2)
            
    return []

def main():
    # 0. å‡†å¤‡è¾“å‡ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“‚ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    temp_save_path = os.path.join(OUTPUT_DIR, TEMP_FILENAME)

    # 1. è¯»å–æ•°æ®
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
        return

    print(f"æ­£åœ¨è¯»å– {INPUT_FILE} ...")
    df = pd.read_excel(INPUT_FILE)
    df['id'] = df['id'].astype(str)

    # ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šéšæœºæŠ½æ ·
    if TEST_MODE:
        print(f"\nğŸ² æµ‹è¯•æ¨¡å¼å·²å¼€å¯ï¼ä½¿ç”¨ç§å­ {RANDOM_SEED} éšæœºæŠ½å– {TEST_K} æ¡æ•°æ®...\n")
        # å¦‚æœæ•°æ®é‡ä¸å¤ŸæŠ½ï¼Œå°±å–å…¨éƒ¨
        n_sample = min(TEST_K, len(df))
        df = df.sample(n=n_sample, random_state=RANDOM_SEED).sort_index() # æŠ½æ ·åæŒ‰åŸåºæ’åˆ—ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    
    # 2. å‡†å¤‡ç»“æœå®¹å™¨
    labeled_results = []
    total_batches = (len(df) // BATCH_SIZE) + 1
    
    print(f"å…± {len(df)} é“é¢˜ç›®ï¼Œå°†åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†ã€‚")
    print("-" * 50)

    # æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶ (ä»…éæµ‹è¯•æ¨¡å¼)
    if os.path.exists(temp_save_path) and not TEST_MODE:
        print(f"æç¤º: æ¸…ç†æ—§ä¸´æ—¶æ–‡ä»¶ {temp_save_path}")
        os.remove(temp_save_path)

    # 3. åˆ†æ‰¹å¤„ç†
    start_time = time.time()
    
    for i in range(0, len(df), BATCH_SIZE):
        batch = df.iloc[i : i + BATCH_SIZE]
        batch_num = (i // BATCH_SIZE) + 1
        
        print(f"ğŸš€ [æ‰¹æ¬¡ {batch_num}/{total_batches}] å¤„ç†é¢˜ç›® {i} - {min(i+BATCH_SIZE, len(df))} ... ", end="")
        
        tags = call_deepseek_api(batch)
        
        if tags:
            labeled_results.extend(tags)
            print(f"âœ… æˆåŠŸ ({len(tags)}æ¡)")
            
            # ğŸ’¾ å®æ—¶ä¿å­˜
            try:
                temp_df = pd.DataFrame(tags)
                temp_df.to_csv(temp_save_path, mode='a', header=not os.path.exists(temp_save_path), index=False)
            except Exception as e:
                print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
        else:
            print("âŒ å¤±è´¥ (è·³è¿‡)")
        
        time.sleep(0.5)

    # 4. ç”Ÿæˆæœ€ç»ˆç»“æœ
    print("-" * 50)
    print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç»“æœæ–‡ä»¶...")
    
    if not labeled_results:
        print("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ ‡ç­¾æ•°æ®ï¼Œç¨‹åºç»“æŸã€‚")
        return

    tags_df = pd.DataFrame(labeled_results)
    tags_df['id'] = tags_df['id'].astype(str)
    
    # æ–‡ä»¶åç”Ÿæˆ
    suffix = f"_{NOTE}" if NOTE else ""
    human_filename = os.path.join(OUTPUT_DIR, f"DINA_Mark_Results{suffix}.xlsx")
    q_matrix_filename = os.path.join(OUTPUT_DIR, f"DINA_Q_Matrix{suffix}.xlsx")

    # ==========================================
    # ç‰ˆæœ¬ A: äººå·¥å¯¹ç…§ç‰ˆ (Human Review) - ä¿æŒä¸å˜
    # ==========================================
    def convert_tags_to_names(tag_list):
        if not isinstance(tag_list, list): return ""
        names = [KNOWLEDGE_MAP.get(tag_id, f"æœªçŸ¥ID_{tag_id}") for tag_id in tag_list]
        return ", ".join(names)

    human_tags_df = tags_df.copy()
    human_tags_df['knowledge_names'] = human_tags_df['tags'].apply(convert_tags_to_names)
    human_tags_df['tags_raw'] = human_tags_df['tags'].apply(lambda x: str(x))
    
    human_final_df = pd.merge(df, human_tags_df[['id', 'knowledge_names', 'tags_raw']], on='id', how='left')
    human_final_df.to_excel(human_filename, index=False)
    print(f"âœ… [1/2] äººå·¥å¯¹ç…§è¡¨å·²ä¿å­˜: {human_filename}")

    # ==========================================
    # ç‰ˆæœ¬ B: Q-Matrix å¢å¼ºç‰ˆ (å¤š Sheet)
    # ==========================================
    knowledge_columns = list(KNOWLEDGE_MAP.values())
    
    # 1. å‡†å¤‡åŸºç¡€ Q çŸ©é˜µæ•°æ®
    for col in knowledge_columns:
        tags_df[col] = 0
        
    for index, row in tags_df.iterrows():
        tag_list = row['tags']
        if isinstance(tag_list, list):
            for tag_id in tag_list:
                if tag_id in KNOWLEDGE_MAP:
                    col_name = KNOWLEDGE_MAP[tag_id]
                    tags_df.at[index, col_name] = 1

    q_cols_to_merge = ['id'] + knowledge_columns
    q_matrix_df = pd.merge(df, tags_df[q_cols_to_merge], on='id', how='left')
    q_matrix_df[knowledge_columns] = q_matrix_df[knowledge_columns].fillna(0).astype(int)

    # 2. å‡†å¤‡â€œè¦†ç›–æƒ…å†µâ€æ•°æ® (Sheet 2)
    coverage_data = []
    for col in knowledge_columns:
        # ç­›é€‰å‡ºå½“å‰çŸ¥è¯†ç‚¹ä¸º 1 çš„è¡Œ
        covered_rows = q_matrix_df[q_matrix_df[col] == 1]
        count = len(covered_rows)
        # è·å–é¢˜ç›® ID åˆ—è¡¨ï¼Œç”¨é€—å·è¿æ¥ (ä¸ºäº†é˜²æ­¢ ID å¤ªé•¿ Excel æ˜¾ç¤ºä¸å…¨ï¼Œè¿™é‡Œåªå­˜å‰ 50 ä¸ª ID ä½œä¸ºç¤ºä¾‹ï¼Œæˆ–è€…å…¨éƒ¨å­˜)
        # è¿™é‡Œæˆ‘å­˜å…¨éƒ¨ ID
        ids_str = ",".join(covered_rows['id'].astype(str).tolist())
        
        coverage_data.append({
            "çŸ¥è¯†ç‚¹": col,
            "è¦†ç›–é¢˜ç›®æ•°": count,
            "é¢˜ç›®IDåˆ—è¡¨": ids_str
        })
    df_coverage = pd.DataFrame(coverage_data)

    # 3. å‡†å¤‡â€œé¢˜ç›®å…³è”æ•°é‡åˆ†å¸ƒâ€æ•°æ® (Sheet 4)
    # è®¡ç®—æ¯è¡Œæœ‰å¤šå°‘ä¸ª 1
    row_sums = q_matrix_df[knowledge_columns].sum(axis=1)
    # ç»Ÿè®¡åˆ†å¸ƒ
    dist_counts = row_sums.value_counts().sort_index()
    df_dist = dist_counts.reset_index()
    df_dist.columns = ['å…³è”çŸ¥è¯†ç‚¹æ•°é‡', 'é¢˜ç›®æ•°']

    # 4. å‡†å¤‡â€œç»Ÿè®¡ä¿¡æ¯â€æ•°æ® (Sheet 3)
    stats_data = {
        "æŒ‡æ ‡": [
            "æ€»é¢˜ç›®æ•°", 
            "æ€»çŸ¥è¯†ç‚¹æ•°", 
            "å¹³å‡æ¯é¢˜å…³è”çŸ¥è¯†ç‚¹æ•°", 
            "æœªåŒ¹é…é¢˜ç›®æ•° (å…³è”æ•°ä¸º0)", 
            "çŸ¥è¯†ç‚¹è¦†ç›–æ•°ä¸­ä½æ•°", 
            "çŸ¥è¯†ç‚¹æœ€å¤§è¦†ç›–æ•°",
            "çŸ¥è¯†ç‚¹æœ€å°è¦†ç›–æ•°"
        ],
        "æ•°å€¼": [
            len(q_matrix_df),
            len(knowledge_columns),
            round(row_sums.mean(), 2),
            (row_sums == 0).sum(),
            df_coverage["è¦†ç›–é¢˜ç›®æ•°"].median(),
            df_coverage["è¦†ç›–é¢˜ç›®æ•°"].max(),
            df_coverage["è¦†ç›–é¢˜ç›®æ•°"].min()
        ]
    }
    df_stats = pd.DataFrame(stats_data)

    # 5. å†™å…¥ Excel (ä½¿ç”¨ ExcelWriter å†™å…¥å¤š Sheet)
    try:
        with pd.ExcelWriter(q_matrix_filename, engine='openpyxl') as writer:
            # Sheet 1: å¿…é¡»æ˜¯ Q çŸ©é˜µï¼Œä¸”æ”¾åœ¨ç¬¬ä¸€ä¸ªï¼Œä¿è¯å…¼å®¹æ€§
            q_matrix_df.to_excel(writer, sheet_name='QçŸ©é˜µ', index=False)
            
            # Sheet 2: è¦†ç›–æƒ…å†µ
            df_coverage.to_excel(writer, sheet_name='è¦†ç›–æƒ…å†µ', index=False)
            
            # Sheet 3: ç»Ÿè®¡ä¿¡æ¯
            df_stats.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
            
            # Sheet 4: å…³è”åˆ†å¸ƒ
            df_dist.to_excel(writer, sheet_name='é¢˜ç›®å…³è”åˆ†å¸ƒ', index=False)
            
        print(f"âœ… [2/2] Q-Matrix å¢å¼ºç‰ˆå·²ä¿å­˜: {q_matrix_filename}")
        print("   (åŒ…å«é¡µç­¾: QçŸ©é˜µ, è¦†ç›–æƒ…å†µ, ç»Ÿè®¡ä¿¡æ¯, é¢˜ç›®å…³è”åˆ†å¸ƒ)")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ Excel å¤±è´¥: {e}")

    # å®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(temp_save_path) and not TEST_MODE:
        os.remove(temp_save_path)
    
    duration = time.time() - start_time
    print("-" * 50)
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶ {duration:.2f} ç§’")
if __name__ == "__main__":
    main()